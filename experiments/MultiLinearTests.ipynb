{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import stack\n",
    "import time\n",
    "import numpy as np\n",
    "import timeit\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MaskedMSELoss(nn.MSELoss):\n",
    "    r\"\"\"Creates a criterion that measures the mean squared error (squared L2 norm) between\n",
    "    each element in the input `x` and target `y`, augmented with target > threshold mask\n",
    "\n",
    "    The loss can be described as:\n",
    "\n",
    "    .. math::\n",
    "        \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "        l_n = \\left( x_n - y_n \\right)^2,\n",
    "\n",
    "    where :math:`N` is the batch size. If reduce is ``True``, then:\n",
    "\n",
    "    .. math::\n",
    "        \\ell(x, y) =\n",
    "        \\begin{cases}\n",
    "            \\operatorname{mean}(L), & \\text{if}\\; \\text{size\\_average} = \\text{True},\\\\\n",
    "            \\operatorname{sum}(L),  & \\text{if}\\; \\text{size\\_average} = \\text{False}.\n",
    "        \\end{cases}\n",
    "\n",
    "    The target is masked using:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        mask = (target > threshold).type(target.type())\n",
    "\n",
    "\n",
    "    The mean square error loss is only evaluated on pixels that are contained in the mask. The output MSE loss\n",
    "    is averaged with the batch size\n",
    "\n",
    "    Args:\n",
    "        reduction is fixed to 'mean' for in-house purposes\n",
    "        input (array-like): The input tensor\n",
    "        target (array-like): The target tensor\n",
    "        threshold (floating point number): Threshold applied to the target under which the loss is not calculated\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *)` where `*` means, any number of additional\n",
    "          dimensions\n",
    "        - Target: :math:`(N, *)`, same shape as the input\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> loss = nn.MSELoss()\n",
    "        >>> input = torch.randn(3, 5, requires_grad=True)\n",
    "        >>> target = torch.randn(3, 5)\n",
    "        >>> output = loss(input, target)\n",
    "        >>> output.backward()\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction='mean', threshold=0):\n",
    "        super(nn.MSELoss, self).__init__(size_average=None, reduce=None, reduction=reduction)\n",
    "        self.reduction = reduction\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        mask = target > self.threshold\n",
    "        out = F.mse_loss(torch.masked_select(input, mask),\n",
    "                         torch.masked_select(target, mask),\n",
    "                         reduction='sum')\n",
    "        if self.reduction == 'sum':\n",
    "            pass\n",
    "        elif self.reduction == 'mean':\n",
    "            out.div_(input.size(0))\n",
    "        else:\n",
    "            raise NotImplementedError('reduction should be sum | mean. Unknown value: {}'.format(self.reduction))\n",
    "        return out\n",
    "\n",
    "\n",
    "def MaskedMSELossFunc(input, target, threshold=0):\n",
    "    r\"\"\"Creates a criterion that measures the mean squared error (squared L2 norm) between\n",
    "    each element in the input `x` and target `y`, augmented with target > threshold mask\n",
    "\n",
    "    The loss can be described as:\n",
    "\n",
    "    .. math::\n",
    "        \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "        l_n = \\left( x_n - y_n \\right)^2,\n",
    "\n",
    "    where :math:`N` is the batch size. If reduce is ``True``, then:\n",
    "\n",
    "    .. math::\n",
    "        \\ell(x, y) =\n",
    "        \\begin{cases}\n",
    "            \\operatorname{mean}(L), & \\text{if}\\; \\text{size\\_average} = \\text{True},\\\\\n",
    "            \\operatorname{sum}(L),  & \\text{if}\\; \\text{size\\_average} = \\text{False}.\n",
    "        \\end{cases}\n",
    "\n",
    "    The target is masked using:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        mask = (target > threshold).type(target.type())\n",
    "\n",
    "\n",
    "    The mean square error loss is only evaluated on pixels that are contained in the mask. The output MSE loss\n",
    "    is averaged with the batch size\n",
    "\n",
    "    Args:\n",
    "        reduction is fixed to 'mean' for in-house purposes\n",
    "        input (array-like): The input tensor\n",
    "        target (array-like): The target tensor\n",
    "        threshold (floating point number): Threshold applied to the target under which the loss is not calculated\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *)` where `*` means, any number of additional\n",
    "          dimensions\n",
    "        - Target: :math:`(N, *)`, same shape as the input\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> loss = nn.MSELoss()\n",
    "        >>> input = torch.randn(3, 5, requires_grad=True)\n",
    "        >>> target = torch.randn(3, 5)\n",
    "        >>> output = loss(input, target)\n",
    "        >>> output.backward()\n",
    "    \"\"\"\n",
    "    mask = target > threshold\n",
    "    return F.mse_loss(torch.masked_select(input, mask),\n",
    "                      torch.masked_select(target, mask),\n",
    "                      reduction='sum').div(input.size(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-4.3254e+01,  2.7958e+02, -1.1480e+02,  ..., -1.9091e+02,\n",
       "            2.7754e+02,  2.7731e+01],\n",
       "          [-3.4527e+01,  2.3807e+01,  2.0243e+02,  ...,  4.4729e+02,\n",
       "            6.2034e+02, -1.8864e+02],\n",
       "          [ 1.2877e+02,  2.3461e+02, -5.6008e+02,  ...,  2.4260e+02,\n",
       "            6.4908e+00, -2.2984e+02],\n",
       "          ...,\n",
       "          [ 1.0465e+02, -1.2141e+02, -2.5101e+00,  ...,  1.7338e+02,\n",
       "            2.3382e+02,  2.4418e+02],\n",
       "          [ 1.4969e+02, -5.8253e+01,  2.1739e+02,  ...,  2.2368e+02,\n",
       "           -9.2690e+00,  9.1248e+01],\n",
       "          [-6.9034e+00,  1.3042e+02,  1.5116e+02,  ...,  1.9448e+01,\n",
       "            2.6557e+02,  5.3585e+02]]],\n",
       "\n",
       "\n",
       "        [[[ 3.9849e+02,  1.7899e+02, -5.0961e+01,  ..., -3.1241e+01,\n",
       "           -1.3056e+02, -1.9713e+02],\n",
       "          [-2.6426e+02, -1.7851e+02,  6.7901e+02,  ..., -1.5844e+02,\n",
       "           -2.2703e+02,  2.3398e+02],\n",
       "          [ 2.7362e+02,  6.6530e+00, -2.2326e+02,  ...,  1.6439e+02,\n",
       "           -1.3200e+02, -2.2242e+02],\n",
       "          ...,\n",
       "          [ 2.7082e+02,  3.6120e+02,  9.1893e+01,  ...,  3.1264e+02,\n",
       "            2.0722e+02,  7.0462e+01],\n",
       "          [ 4.1905e+02, -1.7240e+01, -2.4964e+01,  ...,  2.2251e+02,\n",
       "            2.3997e+02,  2.6074e+02],\n",
       "          [ 1.8497e+02, -5.8269e+01, -2.2487e+02,  ...,  1.0867e+02,\n",
       "            1.1609e+02, -3.3768e+02]]],\n",
       "\n",
       "\n",
       "        [[[-2.5647e+02, -3.6214e+02,  4.8331e+01,  ...,  8.2762e+01,\n",
       "            5.5772e+02, -8.6042e+01],\n",
       "          [ 4.2075e+02,  2.9268e+02,  3.3040e+02,  ..., -1.7178e+02,\n",
       "           -2.6606e+02, -6.0312e+01],\n",
       "          [ 6.0868e+01,  1.7564e+02,  1.9863e+01,  ..., -1.3919e+02,\n",
       "           -8.6051e+00,  2.4143e+02],\n",
       "          ...,\n",
       "          [ 2.7776e+02, -7.6554e+01, -5.9046e+02,  ...,  5.7089e+01,\n",
       "           -4.1740e+01,  1.0438e+02],\n",
       "          [-1.7091e+02,  2.4163e+02,  3.0671e+02,  ..., -2.7236e+02,\n",
       "            1.6818e+02,  2.3361e+02],\n",
       "          [-6.2074e+02, -4.1707e+02,  1.3657e+01,  ...,  5.8982e+01,\n",
       "            5.1292e+01,  2.3095e+02]]],\n",
       "\n",
       "\n",
       "        [[[ 3.7753e+01, -1.0078e+01,  3.0321e+02,  ...,  5.4552e+00,\n",
       "           -7.2322e+02, -1.4939e+02],\n",
       "          [-5.1124e+02, -6.7055e+01,  3.2588e+02,  ...,  1.0675e+02,\n",
       "           -1.5271e+01, -3.1283e+02],\n",
       "          [ 2.9833e+02, -8.8069e+01, -1.3298e+01,  ..., -6.8107e+02,\n",
       "           -3.4848e+02,  1.1979e+02],\n",
       "          ...,\n",
       "          [-3.4050e+02,  2.1148e+02,  1.2671e+02,  ...,  1.2695e+02,\n",
       "           -3.0299e+01,  7.7375e+01],\n",
       "          [-1.0151e+02, -4.5715e+02,  2.0990e+02,  ..., -5.1525e+01,\n",
       "           -8.3443e+01, -6.6911e+01],\n",
       "          [ 6.9733e+01,  1.2231e+01, -3.3221e+02,  ...,  4.8148e+02,\n",
       "            1.4727e+02,  4.9893e+00]]]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.mul_(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEKCAYAAAAcgp5RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = torch.randn(4,1,64,64)\n",
    "t = torch.randn(4,1,64,64)\n",
    "\n",
    "loss = MaskedMSELoss(reduction='mean', threshold=0.0)\n",
    "\n",
    "times = np.zeros((5000, 2))\n",
    "for idx in range(5000):\n",
    "    \n",
    "    s1 = time.time()\n",
    "    loss1 = loss(i,t)\n",
    "    s2 = time.time()\n",
    "    loss2 = MaskedMSELossFunc(i,t, threshold=0.0)\n",
    "    s3 = time.time()\n",
    "    times[idx, 0] = 1000000 * (s2-s1)\n",
    "    times[idx, 1] = 1000000 * (s3-s2)\n",
    "\n",
    "plt.hist(times[:,0]-times[:,1], bins=100, range=(-20, 20), histtype='bar')\n",
    "plt.title('gain in time in $\\mu s$')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import stack\n",
    "from fetalnav.models.spn_models import vgg11_sp, vgg13_sp\n",
    "\n",
    "model = vgg11_sp(num_classes=7, num_maps=512, batch_norm=True, in_channels=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelcuda = torch.nn.DataParallel(model).cuda()\n",
    "input_batch = torch.randn(4, 1, 112, 112)\n",
    "o = modelcuda(Variable(input_batch))\n",
    "print(o.shape)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = 0.\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiLinearMultiLabelSoftMarginLoss(nn.MultiLabelSoftMarginLoss):\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        out = 0.\n",
    "        for idx in range(input.size(1)):\n",
    "            class_input  = input[:, idx]\n",
    "            class_target = target[:, idx]\n",
    "            v = F.multilabel_soft_margin_loss(class_input, class_target, weight=self.weight)\n",
    "            out += v\n",
    "        return out\n",
    "\n",
    "t = torch.zeros(o.size(0), o.size(1))\n",
    "t[0,0] = t[0,1] = 1.\n",
    "t[1,1] = t[1,2] = 1.\n",
    "t[2,2] = t[1,3] = 1.\n",
    "t[3,3] = t[1,4] = 1.\n",
    "state = {}\n",
    "state['output']= o.data.cpu()\n",
    "state['target']= t\n",
    "\n",
    "\n",
    "loss = MultiLinearMultiLabelSoftMarginLoss()\n",
    "\n",
    "loss(Variable(state['output']), Variable(state['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from scipy.misc import imresize\n",
    "from scipy.ndimage import label\n",
    "from spn.modules import SoftProposal\n",
    "\n",
    "# helper functions\n",
    "\n",
    "def hook_spn(model):\n",
    "    if not (hasattr(model, 'sp_hook') and hasattr(model, 'fc_hook')):\n",
    "        model._training = model.training\n",
    "        model.train(False)\n",
    "        \n",
    "        def _pre_sp_hook(self, input, output):\n",
    "            print(self.name)\n",
    "            self.parent_modules[0].class_response_maps = output\n",
    "        def sp_hook(self, input, output):\n",
    "            print(self.name)\n",
    "            self.parent_modules[0].class_response_maps = output\n",
    "        def _fc_hook(self, input, output):\n",
    "            if hasattr(self.parent_modules[0], 'class_response_maps'):\n",
    "                self.parent_modules[0].class_response_maps = F.conv2d(self.parent_modules[0].class_response_maps, self.weight.unsqueeze(-1).unsqueeze(-1))\n",
    "            else:\n",
    "                raise RuntimeError('The SPN is broken, please recreate it.')\n",
    "                \n",
    "        sp_layers = []\n",
    "        fc_layers = []\n",
    "        for mod in model.modules():\n",
    "            if isinstance(mod, SoftProposal):\n",
    "                sp_layers.append(mod)\n",
    "            elif isinstance(mod, torch.nn.Linear):\n",
    "                fc_layers.append(mod)\n",
    "        \n",
    "        if not len(sp_layers) or not len(sp_layers):\n",
    "            raise RuntimeError('Invalid SPN model')\n",
    "        else:\n",
    "            for s in sp_layers:\n",
    "                s.parent_modules = [model]\n",
    "                model.sp_hook = s.register_forward_hook(_sp_hook)\n",
    "            for s in sp_layers:\n",
    "                s.parent_modules = [model]\n",
    "                model.fc_hook = s.register_forward_hook(_fc_hook)\n",
    "    return model\n",
    "\n",
    "def generate_outputs(model, in_var):\n",
    "\n",
    "    from spn import hook_spn\n",
    "    from torch.nn import functional as F\n",
    "\n",
    "    if in_var.ndimension() == 3:\n",
    "        input = in_var.unsqueeze(0)\n",
    "    assert in_var.size(0) == 1, 'Batch processing is currently not supported'\n",
    "    # enable spn inference mode\n",
    "    model = hook_spn(model)\n",
    "    # predict scores\n",
    "    scores = torch.nn.Softmax(dim=1)(model(in_var)).data.cpu().squeeze()\n",
    "    # instantiate maps\n",
    "    maps = F.upsample(model.class_response_maps, size=(in_var.size(2), in_var.size(3)), mode='bilinear').data\n",
    "    return scores.numpy(), maps\n",
    "\n",
    "input_batch = torch.randn(1, 1, 112, 112)\n",
    "a = generate_outputs(model, Variable(input_batch.cuda()))\n",
    "\n",
    "a[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Linear(10,2)\n",
    "a.named_parameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
