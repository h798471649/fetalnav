{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training fetalnav\n",
    "\n",
    "This notebook shows an example of a training session for the paper:\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{toussaint.dlmia.18,\n",
    "    author = {Toussaint, Nicolas and Khanal, Bishesh and Sinclair, Matthew and Gomez, Alberto and Skelton, Emily and Matthew, Jacqueline and Schnabel, Julia A.},\n",
    "    title = {Weakly Supervised Localisation for Fetal Ultrasound Images},\n",
    "    booktitle = {Proceedings of the 4th Workshop on Deep Learning in Medical Image Analysis},\n",
    "    year = {2018}\n",
    "}\n",
    "```\n",
    "\n",
    "## Requirements\n",
    "\n",
    "* Cuda >= 8\n",
    "* Python>=3.5\n",
    "* PyTorch: `pip3 install torch torchvision`\n",
    "* Other Packages: [torchnet](https://github.com/pytorch/tnt), [spn](https://github.com/yeezhu/SPN.pytorch)\n",
    "* [fetalnav](https://github.com/ntoussaint/fetalnav)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# general\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', '.*deprecated.*')\n",
    "\n",
    "# tensorboardX\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# torch - torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms as torchtransforms\n",
    "\n",
    "# transforms\n",
    "from fetalnav.transforms import itk_transforms as itktransforms\n",
    "from fetalnav.transforms import tensor_transforms as tensortransforms\n",
    "# dataset\n",
    "from fetalnav.datasets.itk_metadata_classification import ITKMetaDataClassification\n",
    "# models\n",
    "from fetalnav.models.spn_models import *\n",
    "\n",
    "# engine\n",
    "import engine as torchengine\n",
    "\n",
    "\n",
    "def define_transforms(hp):\n",
    "\n",
    "    keys=('spacing', 'aspect_ratio', 'image_size','rescale_interval','flip')\n",
    "    for k in keys:\n",
    "        assert(k in hp.keys()), 'hyper parameters should contain the key \\'{}\\''.format(k)\n",
    "\n",
    "    ## create transformation and data augmentation schemes\n",
    "\n",
    "    # spacing is arbitrary\n",
    "    resample = itktransforms.Resample(new_spacing=hp['spacing'])\n",
    "    # transform an ITK image into a numpy array\n",
    "    tonumpy = itktransforms.ToNumpy(outputtype='float')\n",
    "    # transform a numpy array into a torch tensor\n",
    "    totensor = torchtransforms.ToTensor()\n",
    "    # crop to an aspect ratio\n",
    "    crop = tensortransforms.CropToRatio(outputaspect=hp['aspect_ratio'])\n",
    "    # padd to an aspect ratio\n",
    "    padd = tensortransforms.PaddToRatio(outputaspect=hp['aspect_ratio'])\n",
    "    # resize image to fixed size\n",
    "    resize = tensortransforms.Resize(size=hp['image_size'], interp='bilinear')\n",
    "    # rescale tensor to  interval\n",
    "    rescale = tensortransforms.Rescale(interval=hp['rescale_interval'])\n",
    "    # flip image in the y axis\n",
    "    flip = tensortransforms.Flip(axis=2) if hp['flip'] else None\n",
    "\n",
    "    # transforms to apply when learning\n",
    "    train_transform = torchtransforms.Compose(\n",
    "                        [resample,\n",
    "                         tonumpy,\n",
    "                         totensor,\n",
    "                         crop,\n",
    "                         resize,\n",
    "                         rescale,\n",
    "                         flip])\n",
    "\n",
    "    # transforms to apply when validating\n",
    "    val_transform = torchtransforms.Compose(\n",
    "                            [resample,\n",
    "                             tonumpy,\n",
    "                             totensor,\n",
    "                             crop,\n",
    "                             resize,\n",
    "                             rescale])\n",
    "\n",
    "    hp['train_transform'] = train_transform\n",
    "    hp['val_transform'] = val_transform\n",
    "\n",
    "    return hp\n",
    "\n",
    "\n",
    "def define_loaders(hp):\n",
    "\n",
    "    keys=('datadir', 'train_transform','val_transform', 'batch_size', 'num_workers')\n",
    "    for k in keys:\n",
    "        assert(k in hp.keys()), 'hyper parameters should contain the key \\'{}\\''.format(k)\n",
    "\n",
    "    datadir = hp['datadir']\n",
    "    train_transform = hp['train_transform']\n",
    "    val_transform = hp['val_transform']\n",
    "\n",
    "    # load datasets\n",
    "    train_dataset = ITKMetaDataClassification(root=datadir, mode='train',\n",
    "                                              transform=train_transform)\n",
    "    val_dataset   = ITKMetaDataClassification(root=datadir, mode='validate',\n",
    "                                              transform=val_transform)\n",
    "\n",
    "    # estimate the samples' weights\n",
    "    train_cardinality = train_dataset.get_class_cardinality()\n",
    "    val_cardinality = val_dataset.get_class_cardinality()\n",
    "    train_sample_weights = torch.from_numpy(train_dataset.get_sample_weights())\n",
    "    val_sample_weights = torch.from_numpy(val_dataset.get_sample_weights())\n",
    "\n",
    "    print('')\n",
    "    print('train-dataset: ')\n",
    "    for idx, c in enumerate(train_dataset.get_classes()):\n",
    "        print('{}: \\t{}'.format(train_cardinality[idx], c))\n",
    "    print('')\n",
    "    print('validate-dataset: ')\n",
    "    for idx, c in enumerate(val_dataset.get_classes()):\n",
    "        print('{}: \\t{}'.format(val_cardinality[idx], c))\n",
    "    print('')\n",
    "\n",
    "    # class labels\n",
    "    classes_train = train_dataset.get_classes()\n",
    "    classes_val = val_dataset.get_classes()\n",
    "\n",
    "    assert(np.array_equal(classes_train, classes_val)), 'classes differ between train and validation sets'\n",
    "    classes = classes_train\n",
    "    del classes_train, classes_val\n",
    "\n",
    "#     # create samplers weighting samples according to the occurence of their respective class\n",
    "#     train_sampler = torch.utils.data.sampler.WeightedRandomSampler(train_sample_weights,\n",
    "#                                                                    int(np.min(train_cardinality)),\n",
    "#                                                                    replacement=True)\n",
    "\n",
    "    # create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=hp['batch_size'], shuffle=True,\n",
    "                                               num_workers=hp['num_workers'])\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                             batch_size=hp['batch_size'], shuffle=False,\n",
    "                                             num_workers=hp['num_workers'])\n",
    "\n",
    "    hp['train_loader'] = train_loader\n",
    "    hp['val_loader'] = val_loader\n",
    "\n",
    "    hp['classes'] = classes\n",
    "\n",
    "    return hp\n",
    "\n",
    "def define_logging(hp):\n",
    "\n",
    "    keys=('coordinate_system','arch', 'learning_rate', 'batch_size', 'aspect_ratio')\n",
    "    for k in keys:\n",
    "        assert(k in hp.keys()), 'hyper parameters should contain the key \\'{}\\''.format(k)\n",
    "\n",
    "    # define output log directory\n",
    "    p='cs={}-m={}-lr={}-bs={}-spn={}-aspect={}'.format(hp['coordinate_system'],\n",
    "                                                       hp['arch'],\n",
    "                                                       hp['learning_rate'],\n",
    "                                                       hp['batch_size'],\n",
    "                                                       1,\n",
    "                                                       hp['aspect_ratio']\n",
    "                                                      )\n",
    "    p=os.path.join('logs', p)\n",
    "    hp['save_model_path'] = p\n",
    "    hp['Logger'] = SummaryWriter(log_dir=p)\n",
    "    return hp\n",
    "\n",
    "def define_model(hp):\n",
    "    from torch.nn import functional as F\n",
    "\n",
    "    class MultiLinearMultiLabelSoftMarginLoss(nn.MultiLabelSoftMarginLoss):\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            out = 0.\n",
    "            targets = torch.stack([target <= 0, target > 0], dim=2).float()\n",
    "            for idx in range(input.size(1)):\n",
    "                class_input  = input[:, idx, :]\n",
    "                class_target = targets[:, idx, :]\n",
    "                v = F.multilabel_soft_margin_loss(class_input, class_target, weight=self.weight)\n",
    "                out += v\n",
    "            return out\n",
    "\n",
    "    keys=('arch', 'classes', 'learning_rate', 'momentum', 'weight_decay')\n",
    "    for k in keys:\n",
    "        assert(k in hp.keys()), 'hyper parameters should contain the key \\'{}\\''.format(k)\n",
    "\n",
    "    print('asking for model: {}'.format(hp['arch']))\n",
    "    num_classes = len(hp['classes'])\n",
    "\n",
    "    model = None\n",
    "\n",
    "    if   hp['arch'] == 'resnet18':\n",
    "        model = resnet18_sp(num_classes, num_maps=512, in_channels=1)\n",
    "    elif hp['arch'] == 'resnet34':\n",
    "        model = resnet34_sp(num_classes, num_maps=512, in_channels=1)\n",
    "    elif hp['arch'] == 'vgg13':\n",
    "        model = vgg13_sp(num_classes, batch_norm=False, num_maps=512, in_channels=1)\n",
    "    elif hp['arch'] == 'vgg13_bn':\n",
    "        model = vgg13_sp(num_classes, batch_norm=True, num_maps=512, in_channels=1)\n",
    "    elif hp['arch'] == 'vgg16':\n",
    "        model = vgg16_sp(num_classes, batch_norm=False, num_maps=512, in_channels=1)\n",
    "    elif hp['arch'] == 'vgg16_bn':\n",
    "        model = vgg16_sp(num_classes, batch_norm=True, num_maps=512, in_channels=1)\n",
    "    elif hp['arch'] == 'alexnet':\n",
    "        model = alexnet_sp(num_classes, num_maps=512, in_channels=1)\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr=hp['learning_rate'],\n",
    "                                momentum=hp['momentum'],\n",
    "                                weight_decay=hp['weight_decay'])\n",
    "    hp['model'] = model\n",
    "    hp['criterion'] = criterion\n",
    "    hp['optimizer'] = optimizer\n",
    "\n",
    "    return hp\n",
    "\n",
    "def explore_dataset(hp):\n",
    "\n",
    "    keys=('train_loader', 'classes', 'batch_size')\n",
    "    for k in keys:\n",
    "        assert(k in hp.keys()), 'hyper parameters should contain the key \\'{}\\''.format(k)\n",
    "\n",
    "    train_loader = hp['train_loader']\n",
    "\n",
    "    # show an image\n",
    "    def imshow(img):\n",
    "        npimg = img.numpy()\n",
    "        return plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "    dataiter = iter(train_loader)\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = dataiter.next()\n",
    "        break\n",
    "    # show a minibatch\n",
    "    plt.figure(0, figsize=(3*hp['batch_size'],3))\n",
    "    imshow(torchvision.utils.make_grid([im for im in images]))\n",
    "    plt.show()\n",
    "#     print('     '+'           '.join(['{}'.format(labels[j]) for j in range(hp['batch_size'])]))\n",
    "\n",
    "\n",
    "def define_dictionaries(variables):\n",
    "    engine_state = {\n",
    "        'use_gpu': torch.cuda.is_available(),\n",
    "        'evaluate': False,\n",
    "        'start_epoch': 0,\n",
    "        'max_epochs': 30,\n",
    "        'epoch_step':[10,20],\n",
    "        'maximize': True,\n",
    "        'resume': None,\n",
    "        'use_pb': True,\n",
    "    }\n",
    "\n",
    "    p = {\n",
    "        'image_size': [224, 224],\n",
    "        'spacing': [.5, .5, 1.],\n",
    "        'flip': True,\n",
    "        'rescale_interval': [0,1],\n",
    "        'batch_size': 6,\n",
    "        'num_workers': 8,\n",
    "        'learning_rate': 0.01,\n",
    "        'momentum': 0.9,\n",
    "        'weight_decay': 5e-4,\n",
    "        'arch': 'resnet18',\n",
    "        'coordinate_system': 'cart',\n",
    "        'aspect_ratio': 1.,\n",
    "    }\n",
    "\n",
    "    print('Initializing state...')\n",
    "    p.update(engine_state)\n",
    "\n",
    "    print('Updating variables...')\n",
    "    p.update(variables)\n",
    "\n",
    "    print('Defining transforms...')\n",
    "    p = define_transforms(p)\n",
    "\n",
    "    print('Defining dataset...')\n",
    "    p = define_loaders(p)\n",
    "\n",
    "    print('Defining model...')\n",
    "    p = define_model(p)\n",
    "\n",
    "    print('Defining logging...')\n",
    "    p = define_logging(p)\n",
    "\n",
    "    return p\n",
    "\n",
    "def learning(hp):\n",
    "\n",
    "    keys=('model', 'train_loader', 'val_loader', 'criterion', 'optimizer', 'save_model_path')\n",
    "    for k in keys:\n",
    "        assert(k in hp.keys()), 'hyper parameters should contain the key \\'{}\\''.format(k)\n",
    "\n",
    "    model = hp['model']\n",
    "    train_loader = hp['train_loader']\n",
    "    val_loader = hp['val_loader']\n",
    "    criterion = hp['criterion']\n",
    "    optimizer = hp['optimizer']\n",
    "\n",
    "    # instantiate a MultiLabelMAP engine\n",
    "    engine = torchengine.MultiLabelMAPEngine(hp)\n",
    "    print(hp['save_model_path'])\n",
    "    # learn\n",
    "    engine.learning(model, criterion, train_loader, val_loader, optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet directory\n",
    "\n",
    "Use your own dataset there, containing a subdirectory **'train'** and within it a labelled set of images, itk-compatible, containing the key **Label** in the metadata dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datadir = # <directory containing a subdirectory 'train' with itk-readable image files>\n",
    "datadir = '/home/nt08/data/fetalnav-paper/dataset/polar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the hyperparameters using this dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "state = {'batch_size': 7,\n",
    "         'max_epochs': 20,\n",
    "         'image_size': [224] * 2,\n",
    "         'evaluate': False,\n",
    "         'resume': None,\n",
    "         'learning_rate': 0.05,\n",
    "         'epoch_step': range(0, 20, 4),\n",
    "         'momentum': 0.7,\n",
    "         'weight_decay':5e-4,\n",
    "         'datadir': datadir,\n",
    "         'arch': 'resnet18',\n",
    "         'save_model_path': '.',\n",
    "         'coordinate_system':'pol',\n",
    "         'aspect_ratio': 1.5\n",
    "         }\n",
    "state = define_dictionaries(state)\n",
    "\n",
    "explore_dataset(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learning(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "\n",
    "written by Nicolas Toussaint\n",
    "\n",
    "Contact: <nicolas.toussaint@gmail.com>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "ssap_exp_config": {
   "error_alert": "Error Occurs!",
   "initial": [],
   "max_iteration": 1000,
   "recv_id": "",
   "running": [],
   "summary": [],
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
